{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushshift API - Extracting Subreddit Submissions\n",
    "\n",
    "---\n",
    "\n",
    "Our main objective in this notebook is to extract submissions (or posts) from two of our favorite subreddits, [/r/explainlikeim5](https://www.reddit.com/r/explainlikeimfive/) and [/r/legaladvice](https://www.reddit.com/r/legaladvice/) via [Pushshift's API](https://github.com/pushshift/api).\n",
    "\n",
    "This handy RESTful web API, created by the /r/datasets mod team, allows us to extract data from new posts on subreddits.  It allows you to input parameters on your search so that you only get back the data you want -- parameters, such as:\n",
    "\n",
    "- subreddit\n",
    "- title\n",
    "- selftext\n",
    "- author\n",
    "- created_utc\n",
    "\n",
    "In this notebook, we will be primarily working with one function to call, extract, and output a Pandas DataFrame.  The main steps that are covered in the `get_reddit` function are as follows:\n",
    "\n",
    "1.  Create a blank Pandas DataFrame\n",
    "2.  Use Pushshift's URL and specific parameters to get back what we want\n",
    "3.  Call using `requests.get()` and get back a 100-row .json file\n",
    "4.  Extract from the API 49 more times all the while using a throttle timer to be mindful of Reddit's server\n",
    "5.  Append these rows to our main DataFrame\n",
    "6.  Return the DataFrame\n",
    "\n",
    "In the end, we will output this into a csv file and save it in our `/data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largely adapted Paulene's code that she graciously shared!  Thanks Paulene!\n",
    "\n",
    "def get_reddit(subreddit):\n",
    "    '''Webscraping subreddit for 5000 submissions and returning a dataframe'''\n",
    "    \n",
    "    # Instantiating some variables\n",
    "    count = 0\n",
    "    df = pd.DataFrame(None)\n",
    "    \n",
    "    # Pushshift's URL and params\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': 100,\n",
    "        'fields': ['subreddit', 'title', 'selftext', 'author', 'created_utc']}\n",
    "    res = requests.get(url, params)\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    \n",
    "    # Creating Pandas DataFrame for submissions\n",
    "    df_new = pd.DataFrame(data = posts)\n",
    "    df = df.append(df_new)\n",
    "    count += 1\n",
    "    print(f'Round {count}')\n",
    "    \n",
    "    # While count is less than 50, bring in more submissions.\n",
    "    while count < 50:\n",
    "        \n",
    "        params2 = {\n",
    "            'subreddit': subreddit,\n",
    "            'size': 100,\n",
    "            'fields': ['subreddit', 'title', 'selftext', 'author', 'created_utc'],\n",
    "            'before': int(df.iloc[-1, 1])}\n",
    "        res2 = requests.get(url, params2)\n",
    "        data2 = res2.json()\n",
    "        posts2 = data2['data']\n",
    "        \n",
    "        # Creating DataFrame\n",
    "        df_new = pd.DataFrame(data = posts2)\n",
    "        df = df.append(df_new)\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        # Progress printouts:  Code adapted from global lect: NLP I\n",
    "        if (count + 1) % 10 == 0:\n",
    "            print(f'Round {count + 1} of 50.') # printing checks help visualize that the fxn is working\n",
    "        \n",
    "        time.sleep(3)\n",
    "\n",
    "    df.set_index(pd.Index([i for i in range(len(df))]), inplace = True)\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling our function and outputting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Round 10 of 50.\n",
      "Round 20 of 50.\n",
      "Round 30 of 50.\n",
      "Round 40 of 50.\n",
      "Round 50 of 50.\n"
     ]
    }
   ],
   "source": [
    "eli5 = get_reddit('explainlikeimfive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eli5.to_csv('./data/eli5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Round 10 of 50.\n",
      "Round 20 of 50.\n",
      "Round 30 of 50.\n",
      "Round 40 of 50.\n",
      "Round 50 of 50.\n"
     ]
    }
   ],
   "source": [
    "advice = get_reddit('advice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "advice.to_csv('./data/advice.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
