{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models\n",
    "\n",
    "---\n",
    "\n",
    "Our main purpose is use ensemble methods to see if we can get a better score using the `both_subreddits.csv` dataset compared to the models used in [Model 1.ipynb](http://localhost:8888/files/projects/project_3/Model%201.ipynb?_xsrf=2%7Cc86cdfcc%7C986afd5dc3780c1ce5583d55d34e3068%7C1594840744).\n",
    "\n",
    "A neat tool that we have at our disposal is called a decision tree.  Of course, one decision tree that splits the data based on the optimal Gini score at every node might be decent but let's try using the data with more than just one tree to get a better aggregate of scores.\n",
    "\n",
    "This is calling _bagging_ or bootstrap aggregating which exposes different trees to different sub-samples of the data.\n",
    "\n",
    "## Import libraries & load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>selftext_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mwilliams7187</td>\n",
       "      <td>1595208496</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>ELI5: If our Sun is a Star, does that mean mos...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salzal</td>\n",
       "      <td>1595208420</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>Eli5: If wearing a face mask doesn’t reduce ox...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1595208362</td>\n",
       "      <td>None</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>Why are horse shoes necessary when they seem t...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bar_Delicious</td>\n",
       "      <td>1595208332</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>Government</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GarbageMiserable0x0</td>\n",
       "      <td>1595208225</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>Do you want to get free games and prizes in yo...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author  created_utc   selftext          subreddit  \\\n",
       "0        mwilliams7187   1595208496  [removed]  explainlikeimfive   \n",
       "1               salzal   1595208420  [removed]  explainlikeimfive   \n",
       "2            [deleted]   1595208362       None  explainlikeimfive   \n",
       "3        Bar_Delicious   1595208332  [removed]  explainlikeimfive   \n",
       "4  GarbageMiserable0x0   1595208225  [removed]  explainlikeimfive   \n",
       "\n",
       "                                               title  title_word_count  \\\n",
       "0  ELI5: If our Sun is a Star, does that mean mos...                26   \n",
       "1  Eli5: If wearing a face mask doesn’t reduce ox...                21   \n",
       "2  Why are horse shoes necessary when they seem t...                16   \n",
       "3                                         Government                 1   \n",
       "4  Do you want to get free games and prizes in yo...                14   \n",
       "\n",
       "   selftext_word_count  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    1  \n",
       "3                    1  \n",
       "4                    1  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits = pd.read_csv('./data/both_subreddits.csv')\n",
    "subreddits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "1. Binarize our target variable `subreddit`\n",
    "2. Take out 'ELI5:' text from our `title` column\n",
    "3. Combine the `title` column and the `selftext` column into one `text` column*\n",
    "\n",
    "*Number 3 was used after I ran models with just the `title` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits['subreddit'] = subreddits['subreddit'].map({'explainlikeimfive': 1, 'Advice': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits['title'] = subreddits['title'].map(lambda x: x.lower().strip('eli5:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits['text'] = subreddits['title'] + ' ' + subreddits['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>selftext_word_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mwilliams7187</td>\n",
       "      <td>1595208496</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "      <td>if our sun is a star, does that mean most of ...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>if our sun is a star, does that mean most of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>salzal</td>\n",
       "      <td>1595208420</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "      <td>if wearing a face mask doesn’t reduce oxygen ...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>if wearing a face mask doesn’t reduce oxygen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1595208362</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>why are horse shoes necessary when they seem t...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>why are horse shoes necessary when they seem t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bar_Delicious</td>\n",
       "      <td>1595208332</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "      <td>government</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>government [removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GarbageMiserable0x0</td>\n",
       "      <td>1595208225</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1</td>\n",
       "      <td>do you want to get free games and prizes in yo...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>do you want to get free games and prizes in yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author  created_utc   selftext  subreddit  \\\n",
       "0        mwilliams7187   1595208496  [removed]          1   \n",
       "1               salzal   1595208420  [removed]          1   \n",
       "2            [deleted]   1595208362       None          1   \n",
       "3        Bar_Delicious   1595208332  [removed]          1   \n",
       "4  GarbageMiserable0x0   1595208225  [removed]          1   \n",
       "\n",
       "                                               title  title_word_count  \\\n",
       "0   if our sun is a star, does that mean most of ...                26   \n",
       "1   if wearing a face mask doesn’t reduce oxygen ...                21   \n",
       "2  why are horse shoes necessary when they seem t...                16   \n",
       "3                                         government                 1   \n",
       "4  do you want to get free games and prizes in yo...                14   \n",
       "\n",
       "   selftext_word_count                                               text  \n",
       "0                    1   if our sun is a star, does that mean most of ...  \n",
       "1                    1   if wearing a face mask doesn’t reduce oxygen ...  \n",
       "2                    1  why are horse shoes necessary when they seem t...  \n",
       "3                    1                               government [removed]  \n",
       "4                    1  do you want to get free games and prizes in yo...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test, Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = subreddits['title']\n",
    "X = subreddits['text']\n",
    "y = subreddits['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7500,), (7500,))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2500,), (2500,))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5012\n",
       "1    0.4988\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier\n",
    "\n",
    "Bagging or bootstrap aggregating still obviously uses bootstrapping technique where you sample from your data with replacement.  Then the model will build a decision tree on each bootstrapped sample.  Finally, it will \"make predictions by passing a test observation through all trees and developing one aggregate prediction for that observation\" (taken from notes from bagging lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('bag', BaggingClassifier(random_state=777))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   37.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation score: 0.9172\n",
      "Best parameters to use: {}\n",
      "Testing score: 0.9248\n"
     ]
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                  param_grid = pipe_params,\n",
    "                  cv=5,\n",
    "                  verbose = 1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best cross validation score: {gs.best_score_}')\n",
    "print(f'Best parameters to use: {gs.best_params_}')\n",
    "print(f'Testing score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier took too long to run given the following hyperparameter searches so I will just use the out-of-the-box classifier.\n",
    "\n",
    "```python\n",
    "'tfidf__stop_words': [None, 'english'],\n",
    "'tfidf__max_features': [6_000],\n",
    "'bag__n_estimators': [100, 70],\n",
    "#     'bag__max_features': [250, 350]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Random Forests differ from bagging in one simple way.  That is, at each node in the decison process it will see a random subset of features.  Then based on those features, it will decide how to best split the data.  Then it will go down the tree based on hyperparameters that are set until it picks a subreddit.  Finally all votes will be aggregated and the final class will be the one with the most votes.\n",
    "\n",
    "Let's go over some key parameters for tuning (taken mostly from [RandomTreeClassifier docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html):\n",
    "\n",
    "- `n_estimators`:  The number of trees in our forest that will be generated for final voting.\n",
    "- `max_depth`:  The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "- `max_features`:  Since the model will be taking a random subset of features at each node to make our decision, this hyperparameter lets you choose how many features it will look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier(random_state=777))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation score: 0.9395999999999999\n",
      "Best parameters to use: {'rf__max_depth': None, 'rf__max_features': 'log2', 'rf__n_estimators': 100}\n",
      "Testing score: 0.9512\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'rf__n_estimators': [75, 80, 85, 100],\n",
    "    'rf__max_depth': [None, 1, 2],\n",
    "    'rf__max_features': ['auto', 'log2']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=params, cv=5, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best cross validation score: {gs.best_score_}')\n",
    "print(f'Best parameters to use: {gs.best_params_}')\n",
    "print(f'Testing score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below was another Random Forest Classifier model that was fitted only to the **titles** of each subreddit.  Notice the difference in scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation score: 0.9122666666666668\n",
      "Best parameters to use: {'rf__max_depth': None, 'rf__max_features': 'log2', 'rf__n_estimators': 100}\n",
      "Testing score: 0.9192\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'rf__n_estimators': [75, 80, 85, 100],\n",
    "    'rf__max_depth': [None, 1, 2],\n",
    "    'rf__max_features': ['auto', 'log2']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=params, cv=5, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best cross validation score: {gs.best_score_}')\n",
    "print(f'Best parameters to use: {gs.best_params_}')\n",
    "print(f'Testing score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "Boosting is another type of ensemble algorithm.  It takes a weak \"base learner\" and iterates on this learner to become better and better.  It does this by weighting or emphasizing mistakes that it made in the previous model.  The key difference between bagging versus boosting is that boosting uses the previous model whereas bagging does not, which is summarized succinctly in this [Quora thread](https://www.quora.com/What-are-the-pros-and-cons-of-bagging-versus-boosting-for-ensemble-machine-learning-techniques?share=1).\n",
    "\n",
    "Because Adaboost cannot be parallelized, we will be working with a `RandomSearchCV` to find better hyperparameters instead of tuning over `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(max_features=1000)),\n",
    "    ('ada', AdaBoostClassifier(random_state=777))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation score: 0.9457333333333333\n",
      "Best parameters to use: {'ada__n_estimators': 200, 'ada__learning_rate': 0.4, 'ada__base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')}\n",
      "Testing score: 0.9472\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'ada__base_estimator': [DecisionTreeClassifier(max_depth=2), DecisionTreeClassifier(max_depth=4)],\n",
    "    'ada__n_estimators': [100, 200, 300, 70],\n",
    "    'ada__learning_rate': [.4, .45, 1, .1]\n",
    "}\n",
    "\n",
    "rand = RandomizedSearchCV(pipe,\n",
    "                          params, \n",
    "                          n_iter=10, \n",
    "                          cv=5, \n",
    "                          scoring='accuracy',\n",
    "                          verbose=1)\n",
    "rand.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best cross validation score: {rand.best_score_}')\n",
    "print(f'Best parameters to use: {rand.best_params_}')\n",
    "print(f'Testing score: {rand.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below was a Decision Tree Classifier model that was fitted to only the **titles** of each subreddit.  Notice the difference in scores and also runtimes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross validation score: 0.8939999999999999\n",
      "Best parameters to use: {'ada__n_estimators': 300, 'ada__learning_rate': 0.1, 'ada__base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')}\n",
      "Testing score: 0.8956\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'ada__base_estimator': [DecisionTreeClassifier(max_depth=2), DecisionTreeClassifier(max_depth=4)],\n",
    "    'ada__n_estimators': [100, 200, 300, 70],\n",
    "    'ada__learning_rate': [.4, .45, 1, .1]\n",
    "}\n",
    "\n",
    "rand = RandomizedSearchCV(pipe,\n",
    "                          params, \n",
    "                          n_iter=10, \n",
    "                          cv=5, \n",
    "                          scoring='accuracy',\n",
    "                          verbose=1)\n",
    "rand.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best cross validation score: {rand.best_score_}')\n",
    "print(f'Best parameters to use: {rand.best_params_}')\n",
    "print(f'Testing score: {rand.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Conclusion:\n",
    "\n",
    "The score using the `selftext` and `title` column did better than using the `title` only.  This makes sense.  The longer runtime makes sense as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
